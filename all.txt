 	README.md 	
	dotproduct.c 	
	helloworld.c 	
	lagrange.c 	
	matmul.c 	
	matmul2dwraparoundmesh.c 	
	matvec.c 	
	oddeventr.c 	
	prefix2dmesh.c 	
	shufexch.c 	
	simpson.c 	
	sumofarray.c 	
	trap.c 	
  


dotproduct.c

#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#define VECLEN 100

int main(int argc, char *argv[])
{
    int i, myid, numprocs, len = VECLEN;
    double *x, *y;
    double mysum, allsum;
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);
    MPI_Comm_rank(MPI_COMM_WORLD, &myid);


    if (myid == 0)
        printf("Starting omp_dotprod_mpi. Using %d tasks...\n", numprocs);


    x = (double *)malloc(len * sizeof(double));
    y = (double *)malloc(len * sizeof(double));

    for (i = 0; i < len; i++)
    {
        x[i] = 1.0;
        y[i] = x[i];
    }

    mysum = 0.0;
    for (i = 0; i < len; i++)
    {
        mysum += x[i] * y[i];
    }

    printf("Task %d partial sum = %f\n", myid, mysum);

    MPI_Reduce(&mysum, &allsum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
    if (myid == 0)
        printf("Done. MPI version: global sum  =  %f \n", allsum);

    free(x);
    free(y);
    MPI_Finalize();
}
/*
#include "mpi.h"
#include<stdio.h>
#include<unistd.h>
#include<stdlib.h>

#define n 10

int arr[] = {1,2,3,4,5,6,7,8,9,10};
int temp_arr[1000];
int brr[] = {1,2,3,4,5,6,7,8,9,10};
int temp_brr[1000];

int main(int argc,char *argv[])
{
    int pid,np,n_er;
    MPI_Status status;

    MPI_Init(&argc,&argv);
    MPI_Comm_size(MPI_COMM_WORLD,&np);
    MPI_Comm_rank(MPI_COMM_WORLD,&pid);
    printf("%d\n",pid);
    if(pid==0)
    {
        int index,i;
        int epp = n/np;
        if(np>1)
        {
            for(i=1;i<np-1;i++)
            {
                index = i*epp;
                MPI_Send(&epp,1,MPI_INT,i,0,MPI_COMM_WORLD);
                MPI_Send(&arr[index],epp,MPI_INT,i,0,MPI_COMM_WORLD);
                MPI_Send(&brr[index],epp,MPI_INT,i,0,MPI_COMM_WORLD);
            }
            index = i*epp;
            int elements_left = n - index;
            MPI_Send(&elements_left,1,MPI_INT,i,0,MPI_COMM_WORLD);
            MPI_Send(&arr[index],elements_left,MPI_INT,i,0,MPI_COMM_WORLD);
            MPI_Send(&brr[index],elements_left,MPI_INT,i,0,MPI_COMM_WORLD);
        }
        int sum = 0;
        for(i=0;i<epp;i++)
            sum += arr[i]*brr[i];
        for(i=1;i<np;i++)
        {
            int temp;
            MPI_Recv(&temp,1,MPI_INT,MPI_ANY_SOURCE,0, MPI_COMM_WORLD,&status);
            sum = sum + temp;
        }
        printf("sum is: %d",sum);
    }
    else
    {
        MPI_Recv(&n_er,1,MPI_INT, 0,0,MPI_COMM_WORLD,&status);
        MPI_Recv(&temp_arr,n_er,MPI_INT, 0,0,MPI_COMM_WORLD,&status);
        MPI_Recv(&temp_brr,n_er,MPI_INT, 0,0,MPI_COMM_WORLD,&status);
        int partial_sum = 0;
        for(int i=0;i<n_er;i++)
            partial_sum+= temp_arr[i]*temp_brr[i];
        MPI_Send(&partial_sum,1,MPI_INT,0,0,MPI_COMM_WORLD);
    }
    MPI_Finalize();
    return 0;
}
*/







helloworld.c

// Hello world program
#include "mpi.h"
#include <stdio.h>
#include <stdlib.h>
#define MASTER 0

int main(int argc, char *argv[])
{
    int numtasks, taskid, len;
    char hostname[MPI_MAX_PROCESSOR_NAME];
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);
    MPI_Comm_rank(MPI_COMM_WORLD, &taskid);
    MPI_Get_processor_name(hostname, &len);
    printf("Hello from task %d on %s!\n", taskid, hostname);
    if (taskid == MASTER)
        printf("MASTER: Number of MPI tasks is: %d\n", numtasks);
    MPI_Finalize();
}






lagrange.c

#include <stdio.h>
#include <string.h>
#include "mpi.h"
void main(int argc, char *argv[])
{
    int my_rank, p, source, dest, tag1 = 1, tag2 = 2, tag3 = 3, tag4 = 4, tag5 = 5, tag6 = 6, tag7 = 7;
    //char message[100];
    MPI_Status status;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &p);
    float a[100];
    float b[100];
    int n;
    if (my_rank == 0)
    {
        printf("enter number of elemenets: \n");
        scanf("%d", &n);
        int i;
        //      printf("enter elements: ");
        for (i = 0; i < n; i++)
        {
            a[i] = i + 1;
            b[i] = (n / 2) - i;
            printf("%f,%f\n", a[i], b[i]);
        }
        float x;
        printf("enter value of x:\n");
        scanf("%f", &x);
        int c = n / p;
        int rem = n - (c * (p - 1));
        float msum = 0.0;
        int j;
        for (i = 0; i < rem; i++)
        {
            float z = 1.0;
            for (j = 0; j < n; j++)
            {
                if (j != i)
                {
                    z = (float)z * ((x - a[j]) / (a[i] - a[j]));
                }
            }
            msum = msum + z * b[i];
        }
        for (dest = 1; dest < p; dest++)
        {
            //int b[20];
            i = rem;
            //for(i=rem;i<rem+c;i++)
            //b[k++]=a[i];
            rem = rem + c;
            MPI_Send(&i, 1, MPI_INT, dest, tag1, MPI_COMM_WORLD);
            MPI_Send(&c, 1, MPI_INT, dest, tag2, MPI_COMM_WORLD);
            MPI_Send(&x, 1, MPI_FLOAT, dest, tag5, MPI_COMM_WORLD);
            MPI_Send(a, 100, MPI_FLOAT, dest, tag4, MPI_COMM_WORLD);
            MPI_Send(b, 100, MPI_FLOAT, dest, tag6, MPI_COMM_WORLD);
            MPI_Send(&n, 1, MPI_INT, dest, tag7, MPI_COMM_WORLD);
            float y;
            MPI_Recv(&y, 1, MPI_FLOAT, dest, tag3, MPI_COMM_WORLD, &status);
            msum += y;
        }
        printf("function value is %f\n", msum);
    }
    else
    {
        source = 0;
        //int rec[100];
        int i, c, n;
        float x;
        MPI_Recv(&i, 1, MPI_INT, source, tag1, MPI_COMM_WORLD, &status);
        MPI_Recv(&c, 1, MPI_INT, source, tag2, MPI_COMM_WORLD, &status);
        MPI_Recv(&x, 1, MPI_FLOAT, source, tag5, MPI_COMM_WORLD, &status);
        MPI_Recv(a, 100, MPI_FLOAT, source, tag4, MPI_COMM_WORLD, &status);
        MPI_Recv(b, 100, MPI_FLOAT, source, tag6, MPI_COMM_WORLD, &status);
        MPI_Recv(&n, 1, MPI_INT, source, tag7, MPI_COMM_WORLD, &status);
        int j, k;
        float ssum = 0.0;
        for (k = i; k < i + c; k++)
        {
            float z = 1.0;
            for (j = 0; j < n; j++)
            {
                if (k != j)
                {
                    z = (float)z * ((x - a[j]) / (a[k] - a[j]));
                }
            }
            ssum += z * b[k];
        }
        MPI_Send(&ssum, 1, MPI_FLOAT, source, tag3, MPI_COMM_WORLD);
    }
    MPI_Finalize();
}






matmul2dwraparoundmesh.c

#include <stdio.h>
#include <string.h>
#include <math.h>
#include "mpi.h"

void main(int argc, char *argv[])
{
    int my_rank, p, a[100][100], b[100][100], source, dest;
    int tag = 0;
    MPI_Status status;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &p);
    if (my_rank != 0)
    {
        int x, y, z, x1, y1, z1;
        MPI_Recv(&x, 1, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);
        MPI_Recv(&y, 1, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);
        MPI_Recv(&z, 1, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);
        int n = sqrt(p - 1);
        int k;
        int i = (my_rank / n) + 1;
        if (my_rank % n == 0)
            i--;
        int j = (my_rank % n);
        if (my_rank % n == 0)
            j = n;
        for (k = 1; k <= n - 1; k++)
        {

            if (i > k)
            {
                int dest = my_rank - 1;
                if (dest % n == 0)
                    dest = dest + n;
                int source = my_rank + 1;
                if (source % n == 1)
                    source = source - n;
                MPI_Send(&x, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);
                MPI_Recv(&x, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &status);
            }
            if (j > k)
            {
                int dest = my_rank - n;
                if (dest <= 0)
                    dest = dest + n * n;
                int source = my_rank + n;
                if (source > n * n)
                    source = source - n * n;
                MPI_Send(&y, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);
                MPI_Recv(&y, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &status);
            }
        }
        for (k = 1; k <= n; k++)
        {
            z = z + x * y;
            int dest = my_rank - 1;
            if (dest % n == 0)
                dest = dest + n;
            int source = my_rank + 1;
            if (source % n == 1)
                source = source - n;
            MPI_Send(&x, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);
            MPI_Recv(&x, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &status);

            dest = my_rank - n;
            if (dest <= 0)
                dest = dest + n * n;
            source = my_rank + n;
            if (source > n * n)
                source = source - n * n;
            MPI_Send(&y, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);
            MPI_Recv(&y, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &status);
        }
        MPI_Send(&z, 1, MPI_INT, 0, tag, MPI_COMM_WORLD);
    }
    else
    {
        int n;
        printf("Enter dimension of matrix: \n");
        scanf("%d", &n);
        int i, j;
        int count = 0;
        for (i = 0; i < n; i++)
        {
            for (j = 0; j < n; j++)
            {
                count++;
                if (j == i)
                    a[i][j] = 1;
                else
                    a[i][j] = 0;

                b[i][j] = 2;
                int x = a[i][j];
                int y = b[i][j];
                int z = 0;
                MPI_Send(&x, 1, MPI_INT, count, tag, MPI_COMM_WORLD);
                MPI_Send(&y, 1, MPI_INT, count, tag, MPI_COMM_WORLD);
                MPI_Send(&z, 1, MPI_INT, count, tag, MPI_COMM_WORLD);
            }
        }
        int c[100][100];
        for (i = 1; i <= n; i++)
        {
            for (j = 1; j <= n; j++)
            {
                int source = (i - 1) * n + j;
                int x;
                MPI_Recv(&x, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &status);
                c[i - 1][j - 1] = x;
                printf("%d ", x);
            }
            printf("\n");
        }
    }
    MPI_Finalize();
}






matmul.c

#include <stdio.h>
#include "mpi.h"
#include <sys/time.h>
#define N 4 /* number of rows and columns in matrix */

MPI_Status status;

double a[N][N], b[N][N], c[N][N];

main(int argc, char **argv)
{
    int numtasks, taskid, numworkers, source, dest, rows, offset, i, j, k;

    struct timeval start, stop;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &taskid);
    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);

    numworkers = numtasks - 1;

    /*---------------------------- master ----------------------------*/
    if (taskid == 0)
    {
        for (i = 0; i < N; i++)
        {
            for (j = 0; j < N; j++)
            {
                a[i][j] = 1.0;
                b[i][j] = 2.0;
            }
        }

        gettimeofday(&start, 0);

        /* send matrix data to the worker tasks */
        rows = N / numworkers;
        offset = 0;

        for (dest = 1; dest <= numworkers; dest++)
        {
            MPI_Send(&offset, 1, MPI_INT, dest, 1, MPI_COMM_WORLD);
            MPI_Send(&rows, 1, MPI_INT, dest, 1, MPI_COMM_WORLD);
            MPI_Send(&a[offset][0], rows * N, MPI_DOUBLE, dest, 1, MPI_COMM_WORLD);
            MPI_Send(&b, N * N, MPI_DOUBLE, dest, 1, MPI_COMM_WORLD);
            offset = offset + rows;
        }

        /* wait for results from all worker tasks */
        for (i = 1; i <= numworkers; i++)
        {
            source = i;
            MPI_Recv(&offset, 1, MPI_INT, source, 2, MPI_COMM_WORLD, &status);
            MPI_Recv(&rows, 1, MPI_INT, source, 2, MPI_COMM_WORLD, &status);
            MPI_Recv(&c[offset][0], rows * N, MPI_DOUBLE, source, 2, MPI_COMM_WORLD, &status);
        }

        gettimeofday(&stop, 0);

        printf("Here is the result matrix:\n");
        for (i = 0; i < N; i++)
        {
            for (j = 0; j < N; j++)
                printf("%6.2f   ", c[i][j]);
            printf("\n");
        }

        fprintf(stdout, "Time = %.6f\n\n",
                (stop.tv_sec + stop.tv_usec * 1e-6) - (start.tv_sec + start.tv_usec * 1e-6));
    }

    /*---------------------------- worker----------------------------*/
    if (taskid > 0)
    {
        source = 0;
        MPI_Recv(&offset, 1, MPI_INT, source, 1, MPI_COMM_WORLD, &status);
        MPI_Recv(&rows, 1, MPI_INT, source, 1, MPI_COMM_WORLD, &status);
        MPI_Recv(&a, rows * N, MPI_DOUBLE, source, 1, MPI_COMM_WORLD, &status);
        MPI_Recv(&b, N * N, MPI_DOUBLE, source, 1, MPI_COMM_WORLD, &status);

        /* Matrix multiplication */
        for (k = 0; k < N; k++)
            for (i = 0; i < rows; i++)
            {
                c[i][k] = 0.0;
                for (j = 0; j < N; j++)
                    c[i][k] = c[i][k] + a[i][j] * b[j][k];
            }

        MPI_Send(&offset, 1, MPI_INT, 0, 2, MPI_COMM_WORLD);
        MPI_Send(&rows, 1, MPI_INT, 0, 2, MPI_COMM_WORLD);
        MPI_Send(&c, rows * N, MPI_DOUBLE, 0, 2, MPI_COMM_WORLD);
    }

    MPI_Finalize();
}







matvec.c

#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>

void multi(int Count, float *Sum, float Vec[], float Data[], int Column)
{
    int i = 0, j = 0, k = 0;
    while (i < Count)
    {
        Sum[i] = 0;
        for (j = 0; j < Column; j++)
        {
            Sum[i] = Sum[i] + Data[k] * Vec[j];
            k++;
        }
        i++;
    }
}

int main(int argc, char *argv[])
{
    int rank, size, *sendcount, *displace, *reccount;
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Status status;
    FILE *fp;
    char c;
    int i, j, k = 0, count = 0, row = 0, column = 0;
    float n = 0, *sum, *rec_data, *data, *vec;
    sendcount = (int *)calloc(sizeof(int), size);
    reccount = (int *)calloc(sizeof(int), size);
    displace = (int *)calloc(sizeof(int), size);
    if (rank == 0)
    {
        fp = fopen("matrix.txt", "r");
        while (fscanf(fp, "%f", &n) != -1)
        {
            c = fgetc(fp);
            if (c == '\n')
            {
                row = row + 1;
            }
            count++;
        }
        column = count / row;
        printf("Row=%d column=%d proc=%d\n", row, column, size);
        float mat[row][column];
        fseek(fp, 0, SEEK_SET);
        data = (float *)calloc(sizeof(float), row * column);
        vec = (float *)calloc(sizeof(float), column);
        for (i = 0; i < row; i++)
        {
            for (j = 0; j < column; j++)
            {
                fscanf(fp, "%f", &mat[i][j]);
                data[k] = mat[i][j];
                k++;
            }
        }
        fclose(fp);
        fp = fopen("vector.txt", "r");
        count = 0;
        while (fscanf(fp, "%f", &n) != -1)
        {
            count++;
        }
        printf("length of vector = %d\n", count);
        if (column != count)
        {
            printf("Dimensions do not match.\nCode Terminated");
            MPI_Abort(MPI_COMM_WORLD, 0);
        }
        fseek(fp, 0, SEEK_SET);
        for (i = 0; i < column; i++)
        {
            fscanf(fp, "%f", &vec[i]);
        }
        fclose(fp);
        count = 0;
        while (1)
        {
            for (i = 0; i < size; i++)
            {
                sendcount[i] = sendcount[i] + 1;
                count++;
                if (count == row)
                    break;
            }
            if (count == row)
                break;
        }
        for (i = 1; i < size; i++)
        {
            displace[i] = displace[i - 1] + sendcount[i - 1] * column;
            sendcount[i - 1] = sendcount[i - 1] * column;
        }
        sendcount[size - 1] = sendcount[size - 1] * column;
        for (i = 0; i < size; i++)
            printf("sendcout=%d disp=%d\n", sendcount[i], displace[i]);
    }
    MPI_Bcast(&row, 1, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(&column, 1, MPI_INT, 0, MPI_COMM_WORLD);
    if (rank != 0)
    {
        //data=(float*)calloc(sizeof(float),row*column);
        vec = (float *)malloc(sizeof(float) * column);
    }
    MPI_Bcast(vec, column, MPI_FLOAT, 0, MPI_COMM_WORLD);
    MPI_Bcast(sendcount, size, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(displace, size, MPI_INT, 0, MPI_COMM_WORLD);
    rec_data = (float *)calloc(sizeof(float), sendcount[rank]);
    //MPI_Bcast(data,row*column,MPI_INT,0,MPI_COMM_WORLD);
    MPI_Scatterv(data, sendcount, displace, MPI_FLOAT, rec_data, sendcount[rank], MPI_FLOAT, 0, MPI_COMM_WORLD);
    count = sendcount[rank] / column;
    sum = (float *)calloc(sizeof(float), count);
    multi(count, sum, vec, rec_data, column);
    float *result = (float *)calloc(sizeof(float), row);
    int disp[size];
    disp[0] = 0;
    reccount[0] = sendcount[0] / column;
    for (i = 1; i < size; i++)
    {
        disp[i] = disp[i - 1] + sendcount[i - 1] / column;
        reccount[i] = sendcount[i] / column;
    }
    MPI_Gatherv(sum, count, MPI_FLOAT, result, reccount, disp, MPI_FLOAT, 0, MPI_COMM_WORLD);
    if (rank == 0)
    {
        printf("\nMatrix Vector Multiplication is:\n");
        for (i = 0; i < row; i++)
        {
            printf("%.3f\n", result[i]);
        }
    }
    free(vec);
    free(sum);
    free(sendcount);
    free(displace);
    free(reccount);
    free(rec_data);
    MPI_Finalize();
    return 0;
}


oddeventr.c

#include <stdio.h>
#include <string.h>
#include "mpi.h"

void main(int argc, char *argv[])
{
    int my_rank, p, a[100], source, dest;
    int tag = 0;
    MPI_Status status;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &p);
    if (my_rank != 0)
    {
        int x, z;
        MPI_Recv(&x, 1, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);
        int j;
        int k = (p - 1) / 2;
        if ((p - 1) % 2 != 0)
            k++;
        for (j = 1; j <= k; j++)
        {
            if (my_rank + 1 < p && my_rank % 2 == 1)
            {
                MPI_Send(&x, 1, MPI_INT, my_rank + 1, tag, MPI_COMM_WORLD);
                MPI_Recv(&x, 1, MPI_INT, my_rank + 1, tag, MPI_COMM_WORLD, &status);
            }
            if (my_rank - 1 >= 1 && my_rank % 2 == 0)
            {
                MPI_Recv(&z, 1, MPI_INT, my_rank - 1, tag, MPI_COMM_WORLD, &status);
                if (z > x)
                {
                    int temp = z;
                    z = x;
                    x = temp;
                }
                MPI_Send(&z, 1, MPI_INT, my_rank - 1, tag, MPI_COMM_WORLD);
            }
            if (my_rank + 1 < p && my_rank % 2 == 0)
            {
                MPI_Send(&x, 1, MPI_INT, my_rank + 1, tag, MPI_COMM_WORLD);
                MPI_Recv(&x, 1, MPI_INT, my_rank + 1, tag, MPI_COMM_WORLD, &status);
            }
            if (my_rank - 1 >= 1 && my_rank % 2 == 1)
            {
                MPI_Recv(&z, 1, MPI_INT, my_rank - 1, tag, MPI_COMM_WORLD, &status);
                if (z > x)
                {
                    int temp = z;
                    z = x;
                    x = temp;
                }
                MPI_Send(&z, 1, MPI_INT, my_rank - 1, tag, MPI_COMM_WORLD);
            }
        }
        MPI_Send(&x, 1, MPI_INT, 0, tag, MPI_COMM_WORLD);
    }
    else
    {
        int n;
        printf("Enter total elements: \n");
        scanf("%d", &n);
        int i;
        for (i = 0; i < n; i++)
            scanf("%d", &a[i]);
        for (i = 0; i < n; i++)
        {
            int b = a[i];
            MPI_Send(&b, 1, MPI_INT, i + 1, tag, MPI_COMM_WORLD);
        }
        for (source = 1; source < p; source++)
        {
            int x;
            MPI_Recv(&x, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &status);
            a[source - 1] = x;
            printf("Message from process %d element = %d\n", source - 1, x);
        }
    }
    MPI_Finalize();
}







prefix2dmesh.c

#include <stdio.h>
#include <string.h>
#include <math.h>
#include "mpi.h"
void main(int argc, char *argv[])
{
    int my_rank, p, source, dest, tag1 = 1, tag2 = 2, tag3 = 3, tag4 = 4, tag5 = 5, tag6 = 6, tag7 = 7, tag8 = 8;
    //char message[100];
    MPI_Status status;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &p);
    int a[100];
    int n;
    if (my_rank == 0)
    {
        printf("enter number of elemenets: \n");
        scanf("%d", &n);
        int i;
        //      printf("enter elements: ");
        for (i = 0; i < n; i++)
            a[i] = i + 1;
        int j = 2;
        for (i = 0; j <= n;)
        {
            int f;
            for (dest = 1; dest < p; dest++)
            {
                int y = 1 << i;
                int src = dest - y;
                MPI_Send(&src, 1, MPI_INT, dest, tag6, MPI_COMM_WORLD);
            }
            int x = 1 << i;
            dest = 0 + x;
            MPI_Send(&i, 1, MPI_INT, dest, tag1, MPI_COMM_WORLD);
            MPI_Send(a, 100, MPI_INT, dest, tag4, MPI_COMM_WORLD);
            MPI_Recv(a, 100, MPI_INT, dest, tag3, MPI_COMM_WORLD, &status);
            //for(dest=1;dest<p;dest++)
            //MPI_Recv(&f,1,dest,tag8,MPI_COMM_WORLD,&status);
            i++;
            j = 1 << (i + 1);
        }
        printf("prefix sum in process %d is %d\n", my_rank, a[0]);
    }
    else
    {
        source = 0;
        int src;
        MPI_Recv(&src, 1, MPI_INT, source, tag6, MPI_COMM_WORLD, &status);
        if (src >= 0)
            if (src >= 0)
            {
                source = src;
                int i;
                MPI_Recv(&i, 1, MPI_INT, source, tag1, MPI_COMM_WORLD, &status);
                MPI_Recv(a, 100, MPI_INT, source, tag4, MPI_COMM_WORLD, &status);
                int x = 1 << i;
                dest = my_rank + x;
                if (dest < p)
                {
                    MPI_Send(&i, 1, MPI_INT, dest, tag2, MPI_COMM_WORLD);
                    MPI_Send(a, 100, MPI_INT, dest, tag5, MPI_COMM_WORLD);
                    MPI_Recv(a, 100, MPI_INT, dest, tag7, MPI_COMM_WORLD, &status);
                }
                a[my_rank] += a[source];
                int j = 1 << (i + 1);
                if (j == p)
                {
                    printf("prefix sum in process %d is %d\n", my_rank, a[my_rank]);
                }
                MPI_Send(a, 100, MPI_INT, source, tag3, MPI_COMM_WORLD);
            }
    }
    MPI_Finalize();
}





# pdclab



shufexch.c

#include <stdio.h>
#include <string.h>
#include "mpi.h"
#include <math.h>

int routingFn(int j, int i)
{
    return j + pow(2, i);
}

int routingFn2(int j)
{
    int t;
    t = j >> 2;
    j = (j << 1);
    j = (j % 8) | t;
    return j;
}

void main(int argc, char *argv[])
{
    int my_rank, p, source, dest, tag = 0;
    int tag2 = 1;
    MPI_Status status;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &p);
    int n;
    int my_value;
    int my_step;
    int total_step = log2(p);
    int x = 1;
    if (my_rank == 0)
    {
        my_step = 0;
        int arr[] = {1, 1, 1, 1, 1, 1, 1, 1};
        int mask[] = {0, 1, 0, 1, 0, 1, 0, 1};
        int mask_n[] = {0, 0, 0, 0, 0, 0, 0, 0};
        my_value = arr[0];
        int i;
        for (my_step = 0; my_step < total_step; my_step++)
        {
            for (i = 1; i < p; i++)
            {
                int t = arr[i];
                int m = mask[i];
                MPI_Send(&t, 1, MPI_INT, i, tag, MPI_COMM_WORLD);
                MPI_Send(&m, 1, MPI_INT, i, tag + 1, MPI_COMM_WORLD);
            }

            if (mask[my_rank])
            {
                arr[0] *= x;
            }
            for (i = 1; i < p; i++)
            {
                int y;
                MPI_Recv(&y, 1, MPI_INT, i, tag + 2, MPI_COMM_WORLD, &status);
                arr[i] = y;
            }

            for (i = 0; i < p; i++)
            {
                int t = routingFn2(i);
                mask_n[t] = mask[i];
            }
            for (i = 0; i < p; i++)
            {
                mask[i] = mask_n[i];
                printf("%d ", mask[i]);
            }
            printf("\n");
        }
        my_value = arr[0];

        for (my_step = 0; my_step < total_step; my_step++)
        {
            for (i = 1; i < p; i++)
            {
                int t = arr[i];
                MPI_Send(&t, 1, MPI_INT, i, tag, MPI_COMM_WORLD);
                MPI_Send(&my_step, 1, MPI_INT, i, tag + 1, MPI_COMM_WORLD);
            }

            int ns;
            ns = routingFn(my_rank, my_step);
            int k = arr[my_rank];
            MPI_Send(&k, 1, MPI_INT, ns, tag + 3, MPI_COMM_WORLD);
            for (i = 1; i < p; i++)
            {
                int y;
                MPI_Recv(&y, 1, MPI_INT, i, tag + 2, MPI_COMM_WORLD, &status);

                arr[i] = y;
            }
        }
        for (i = 0; i < p - 1; i++)
        {
            printf("Result for P[%d] : %d\n", i, arr[i]);
        }
    }
    else
    {
        int j;
        for (j = 0; j < total_step; j++)
        {
            source = 0;
            MPI_Recv(&my_value, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &status);
            MPI_Recv(&my_step, 1, MPI_INT, source, tag + 1, MPI_COMM_WORLD, &status);
            if (my_step)
            {
                my_value *= x;
            }
            MPI_Send(&my_value, 1, MPI_INT, source, tag + 2, MPI_COMM_WORLD);
            x *= x;
        }

        for (j = 0; j < total_step; j++)
        {
            source = 0;
            MPI_Recv(&my_value, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &status);
            MPI_Recv(&my_step, 1, MPI_INT, source, tag + 1, MPI_COMM_WORLD, &status);
            int ns;
            if (my_rank % 2 == 0)
            {
                ns = routingFn(my_rank, my_step);
                if (ns < 8)
                {
                    MPI_Send(&my_value, 1, MPI_INT, ns, tag + 3, MPI_COMM_WORLD);
                }
                int incoming;
                int incomingP = my_rank - pow(2, my_step);
                if (my_rank >= pow(2, my_step))
                {
                    MPI_Recv(&incoming, 1, MPI_INT, incomingP, tag + 3, MPI_COMM_WORLD, &status);
                }

                my_value += incoming;

                MPI_Send(&my_value, 1, MPI_INT, source, tag + 2, MPI_COMM_WORLD);
            }
            else
            {
                ns = routingFn(my_rank, my_step);
                int incoming;
                int incomingP = my_rank - pow(2, my_step);
                if (my_rank >= pow(2, my_step))
                {
                    MPI_Recv(&incoming, 1, MPI_INT, incomingP, tag + 3, MPI_COMM_WORLD, &status);
                }
                if (ns < 8)
                {
                    MPI_Send(&my_value, 1, MPI_INT, ns, tag + 3, MPI_COMM_WORLD);
                }
                my_value += incoming;

                MPI_Send(&my_value, 1, MPI_INT, source, tag + 2, MPI_COMM_WORLD);
            }
        }
    }

    MPI_Finalize();
}






simpson.c

#include <stdio.h>
#include <math.h>
#include <mpi.h>

#define approx_val 2.19328059
#define N 32 /* Number of intervals in each processor */

double integrate_f(double); /* Integral function */
double simpson(int, double, double, double);

int main(int argc, char *argv[])
{
    int Procs;   /* Number of processors */
    int my_rank; /* Processor number */
    double total;
    double exact_val_of_Pi, pi, y, processor_output_share[8], x1, x2, l, sum;
    int i;
    MPI_Status status;

    /* Let the system do what it needs to start up MPI */
    MPI_Init(&argc, &argv);

    /* Get my process rank */
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);

    /* Find out how many processes are being used. */
    MPI_Comm_size(MPI_COMM_WORLD, &Procs);

    /* Each processor computes its interval */
    x1 = ((double)my_rank) / ((double)Procs);
    x2 = ((double)(my_rank + 1)) / ((double)Procs);

    /* l is the same for all processes. */
    l = 1.0 / ((double)(2 * N * Procs));
    sum = 0.0;
    for (i = 1; i < N; i++)
    {
        y = x1 + (x2 - x1) * ((double)i) / ((double)N);

        /* call Simpson's rule  */
        sum = (double)simpson(i, y, l, sum);
    }

    /* Include the endpoints of the intervals */
    sum += (integrate_f(x1) + integrate_f(x2)) / 2.0;
    total = sum;

    /* Add up the integrals calculated by each process. */
    if (my_rank == 0)
    {
        processor_output_share[0] = total;

        /* source = i, tag = 0 */
        for (i = 1; i < Procs; i++)
            MPI_Recv(&(processor_output_share[i]), 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);
    }
    else
    {
        /* dest = 0, tag = 0 */
        MPI_Send(&total, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
    }

    /* Add up the value of Pi and print the result.  */
    if (my_rank == 0)
    {
        for (i = 0; i < Procs; i++)
            pi += processor_output_share[i];
        pi *= 2.0 * l / 3.0;
        printf("-------------------------------------------------\n");
        printf("The computed Pi of the integral for %d grid points is  %25.16e \n",
               (N * Procs), pi);

        /* This is directly derived from the integeration of the formula. See 
  the report. */
#if 1
        exact_val_of_Pi = 4.0 * atan(1.0);
#endif

#if 0
      exact_val_of_Pi = 4.0 * log(approx_val);
#endif
        printf("The error or the discrepancy between exact and computed value of Pi : %25.16e\n",
               fabs(pi - exact_val_of_Pi));
        printf("-------------------------------------------------\n");
    }

    MPI_Finalize();
}

double integrate_f(double x)
{
    /* compute and return value */
    return 4.0 / (1.0 + x * x);
}

double simpson(int i, double y, double l, double sum)
{
    /* store result in sum */
    sum += integrate_f(y);
    sum += 2.0 * integrate_f(y - l);
    if (i == (N - 1))
        sum += 2.0 * integrate_f(y + l);
    return sum;
} /* simpson */

/*
#include "mpi.h"
#include<stdlib.h>
#include<unistd.h>
#include<stdio.h>
#include<math.h>

int n;
int N;
int epp;
float f(float x,int i)
{
    if(i==0)
        return 1.0/(x*x+1);
    if(i==n)
        return 1.0/(x*x+1);
    if(i%2)
        return 4.0/(x*x+1);
    return 2.0/(x*x+1);
}

int main(int argc,char *argv[])
{
    int pid,n_er,np;
    float a,b;
    //scanf("%d%f%f",&n,&a,&b);
    n = 6;
    a = 0;
    b = 1;
    N = n+1;
    float h = (b-a)/(float)n;
    MPI_Status status;
    MPI_Init(&argc,&argv);
    MPI_Comm_size(MPI_COMM_WORLD,&np);
    MPI_Comm_rank(MPI_COMM_WORLD,&pid);
    epp = N/np;
    if(pid==0)
    {
        //printf("%d\n",pid);
        //printf("%d %d %d\n",epp,N, np);
        int index,i;
        if(np>1)
        {
            for(i=1;i<np-1;i++)
                MPI_Send(&epp,1,MPI_INT,i,0,MPI_COMM_WORLD);
            //printf("%d",i);   
            index = i*epp;
            int e_left = N - index;
            MPI_Send(&e_left,1,MPI_INT,i,0,MPI_COMM_WORLD);
        }
        float sum = 0;
        for(i=0;i<epp;i++)
        {
            float val = f(a+((float)i)*h,i);
            sum = sum + val;
            //printf("%f %d",val,i);
        }
        for(i=1;i<np;i++)
        {
            float temp;
            MPI_Recv(&temp,1,MPI_FLOAT,MPI_ANY_SOURCE,0,MPI_COMM_WORLD,&status);
            sum = sum + temp;
        }
        float ans = (h*sum)/3.0;
        printf("the value is: %f",ans);
    }
    else
    {
        //printf("%d\n",pid);
        MPI_Recv(&n_er,1,MPI_INT,0,0,MPI_COMM_WORLD,&status);
        float partial_sum = 0;
        int i;
        for(i=0;i<n_er;i++)
        {
            float index = pid*epp + i;
            //printf("%f %d %d\n",index,epp,i);
            float val = f(a+index*(h),index);
            partial_sum +=val;
            //printf("%f %f \n",val,index);
            //printf("%f",partial_sum);
        }
        MPI_Send(&partial_sum,1,MPI_FLOAT,0,0,MPI_COMM_WORLD);    
    }
    MPI_Finalize();
    return 0;
}
*/







sumofarray.c

#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

// size of array
#define n 10

int a[] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 11};

// Temporary array for slave process
int a2[1000];

int main(int argc, char *argv[])
{

    int pid, np,
        elements_per_process,
        n_elements_recieved;
    // np -> no. of processes
    // pid -> process id

    MPI_Status status;

    // Creation of parallel processes
    MPI_Init(&argc, &argv);

    // find out process ID,
    // and how many processes were started
    MPI_Comm_rank(MPI_COMM_WORLD, &pid);
    MPI_Comm_size(MPI_COMM_WORLD, &np);

    // master process
    if (pid == 0)
    {
        int index, i;
        elements_per_process = n / np;

        // check if more than 1 processes are run
        if (np > 1)
        {
            // distributes the portion of array
            // to child processes to calculate
            // their partial sums
            for (i = 1; i < np - 1; i++)
            {
                index = i * elements_per_process;

                MPI_Send(&elements_per_process,
                         1, MPI_INT, i, 0,
                         MPI_COMM_WORLD);
                MPI_Send(&a[index],
                         elements_per_process,
                         MPI_INT, i, 0,
                         MPI_COMM_WORLD);
            }

            // last process adds remaining elements
            index = i * elements_per_process;
            int elements_left = n - index;

            MPI_Send(&elements_left,
                     1, MPI_INT,
                     i, 0,
                     MPI_COMM_WORLD);
            MPI_Send(&a[index],
                     elements_left,
                     MPI_INT, i, 0,
                     MPI_COMM_WORLD);
        }

        // master process add its own sub array
        int sum = 0;
        for (i = 0; i < elements_per_process; i++)
            sum += a[i];

        // collects partial sums from other processes
        int tmp;
        for (i = 1; i < np; i++)
        {
            MPI_Recv(&tmp, 1, MPI_INT,
                     MPI_ANY_SOURCE, 0,
                     MPI_COMM_WORLD,
                     &status);
            int sender = status.MPI_SOURCE;

            sum += tmp;
        }

        // prints the final sum of array
        printf("Sum of array is : %d\n", sum);
    }
    // slave processes
    else
    {
        MPI_Recv(&n_elements_recieved,
                 1, MPI_INT, 0, 0,
                 MPI_COMM_WORLD,
                 &status);

        // stores the received array segment
        // in local array a2
        MPI_Recv(&a2, n_elements_recieved,
                 MPI_INT, 0, 0,
                 MPI_COMM_WORLD,
                 &status);

        // calculates its partial sum
        int partial_sum = 0;
        for (int i = 0; i < n_elements_recieved; i++)
            partial_sum += a2[i];

        // sends the partial sum to the root process
        MPI_Send(&partial_sum, 1, MPI_INT,
                 0, 0, MPI_COMM_WORLD);
    }

    // cleans up all MPI state before exit of process
    MPI_Finalize();

    return 0;
}







trap.c

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <mpi.h>

const double a = 0;
const double b = 2000000000;

/* Function declarations */
void Get_input(int argc, char *argv[], int my_rank, double *n_p);
double Trap(double left_endpt, double right_endpt, int trap_count,
            double base_len);
double f(double x);

int main(int argc, char **argv)
{
    int my_rank, comm_sz, local_n;
    double n, h, local_a, local_b;
    double local_int, total_int;
    double start, finish, loc_elapsed, elapsed;

    MPI_Init(NULL, NULL);
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);

    Get_input(argc, argv, my_rank, &n); /*Read user input */

    /*Note: h and local_n are the same for all processes*/
    h = (b - a) / n;       /* length of each trapezoid */
    local_n = n / comm_sz; /* number of trapezoids per process */

    /* Length of each process' interval of integration = local_n*h. */
    local_a = a + my_rank * local_n * h;
    local_b = local_a + local_n * h;

    MPI_Barrier(MPI_COMM_WORLD);
    start = MPI_Wtime();
    /* Calculate each process' local integral using local endpoints*/
    local_int = Trap(local_a, local_b, local_n, h);
    finish = MPI_Wtime();
    loc_elapsed = finish - start;
    MPI_Reduce(&loc_elapsed, &elapsed, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);

    /* Add up the integrals calculated by each process */
    MPI_Reduce(&local_int, &total_int, 1, MPI_DOUBLE, MPI_SUM, 0,
               MPI_COMM_WORLD);

    if (my_rank == 0)
    {
        printf("With n = %.0f trapezoids, our estimate\n", n);
        printf("of the integral from %.0f to %.0f = %.0f\n",
               a, b, total_int);
        printf("Elapsed time = %f milliseconds \n", elapsed * 1000);
    }

    /* Shut down MPI */
    MPI_Finalize();

    return 0;
} /*  main  */

void Get_input(int argc, char *argv[], int my_rank, double *n_p)
{
    if (my_rank == 0)
    {
        if (argc != 2)
        {
            fprintf(stderr, "usage: mpirun -np <N> %s <number of trapezoids> \n", argv[0]);
            fflush(stderr);
            *n_p = -1;
        }
        else
        {
            *n_p = atoi(argv[1]);
        }
    }
    // Broadcasts value of n to each process
    MPI_Bcast(n_p, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    // negative n ends the program
    if (*n_p <= 0)
    {
        MPI_Finalize();
        exit(-1);
    }
} /* Get_input */

    double Trap(double left_endpt, double right_endpt, int trap_count, double base_len)
{
    double estimate, x;
    int i;

    estimate = (f(left_endpt) + f(right_endpt)) / 2.0;
    for (i = 1; i <= trap_count - 1; i++)
    {
        x = left_endpt + i * base_len;
        estimate += f(x);
    }
    estimate = estimate * base_len;

    return estimate;
} /*  Trap  */

double f(double x)
{
    return x * x;
} /* f */





////////////////////////////// cuda cuda cuda //////////////////////////////
 	README.md 	
	helloworld.cu	
	matmul.cu 	
	matmulremote.cu 	
	sum2numbers.cu 	
	sum2matrix.cu 	
	sum2vectors.cu 	





helloworld.cu

// In file hello.cu:

#include "stdio.h"
int main()
{
 printf("Hello, world\n");
 return 0;
}

// On your host machine, you can compile and this with: 

// $ nvcc hello.cu

// Execution on GPU equipped server
// $ ./a.out






matmul.cu

/*
 *  file name: matrix.cu
 *
 *  matrix.cu contains the code that realize some common used matrix operations in CUDA
 *  
 *  this is a toy program for learning CUDA, some functions are reusable in other project
 *  
 */
#include <stdio.h>
#include <stdlib.h>
#include <assert.h>

#define BLOCK_SIZE 16

/*
*********************************************************************
function name: gpu_matrix_mult

description: dot product of two matrix (not only square)

parameters: 
            &a GPU device pointer to a m X n matrix (A)
            &b GPU device pointer to a n X k matrix (B)
            &c GPU device output purpose pointer to a m X k matrix (C) 
            to store the result

Note:
    grid and block should be configured as:
        dim3 dimGrid((k + BLOCK_SIZE - 1) / BLOCK_SIZE, (m + BLOCK_SIZE - 1) / BLOCK_SIZE);
        dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);

    further sppedup can be obtained by using shared memory to decrease global memory access times
return: none
*********************************************************************
*/
__global__ void gpu_matrix_mult(int *a,int *b, int *c, int m, int n, int k)
{ 
    int row = blockIdx.y * blockDim.y + threadIdx.y; 
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    int sum = 0;
    if( col < k && row < m) 
    {
        for(int i = 0; i < n; i++) 
        {
            sum += a[row * n + i] * b[i * k + col];
        }
        c[row * k + col] = sum;
    }
} 

/*
*********************************************************************
function name: gpu_square_matrix_mult

description: dot product of two matrix (not only square) in GPU

parameters: 
            &a GPU device pointer to a n X n matrix (A)
            &b GPU device pointer to a n X n matrix (B)
            &c GPU device output purpose pointer to a n X n matrix (C) 
            to store the result
Note:
    grid and block should be configured as:

        dim3 dim_grid((n - 1) / BLOCK_SIZE + 1, (n - 1) / BLOCK_SIZE + 1, 1);
        dim3 dim_block(BLOCK_SIZE, BLOCK_SIZE, 1);

return: none
*********************************************************************
*/
__global__ void gpu_square_matrix_mult(int *d_a, int *d_b, int *d_result, int n) 
{
    __shared__ int tile_a[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ int tile_b[BLOCK_SIZE][BLOCK_SIZE];

    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;
    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;
    int tmp = 0;
    int idx;

    for (int sub = 0; sub < gridDim.x; ++sub) 
    {
        idx = row * n + sub * BLOCK_SIZE + threadIdx.x;
        if(idx >= n*n)
        {
            // n may not divisible by BLOCK_SIZE
            tile_a[threadIdx.y][threadIdx.x] = 0;
        }
        else
        {
            tile_a[threadIdx.y][threadIdx.x] = d_a[idx];
        }

        idx = (sub * BLOCK_SIZE + threadIdx.y) * n + col;
        if(idx >= n*n)
        {
            tile_b[threadIdx.y][threadIdx.x] = 0;
        }  
        else
        {
            tile_b[threadIdx.y][threadIdx.x] = d_b[idx];
        }
        __syncthreads();

        for (int k = 0; k < BLOCK_SIZE; ++k) 
        {
            tmp += tile_a[threadIdx.y][k] * tile_b[k][threadIdx.x];
        }
        __syncthreads();
    }
    if(row < n && col < n)
    {
        d_result[row * n + col] = tmp;
    }
}

/*
*********************************************************************
function name: gpu_matrix_transpose

description: matrix transpose

parameters: 
            &mat_in GPU device pointer to a rows X cols matrix
            &mat_out GPU device output purpose pointer to a cols X rows matrix 
            to store the result
Note:
    grid and block should be configured as:
        dim3 dim_grid((n - 1) / BLOCK_SIZE + 1, (n - 1) / BLOCK_SIZE + 1, 1);
        dim3 dim_block(BLOCK_SIZE, BLOCK_SIZE, 1);

return: none
*********************************************************************
*/
__global__ void gpu_matrix_transpose(int* mat_in, int* mat_out, unsigned int rows, unsigned int cols) 
{
    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int idy = blockIdx.y * blockDim.y + threadIdx.y;

    if (idx < cols && idy < rows) 
    {
        unsigned int pos = idy * cols + idx;
        unsigned int trans_pos = idx * rows + idy;
        mat_out[trans_pos] = mat_in[pos];
    }
}
/*
*********************************************************************
function name: cpu_matrix_mult

description: dot product of two matrix (not only square) in CPU, 
             for validating GPU results

parameters: 
            &a CPU host pointer to a m X n matrix (A)
            &b CPU host pointer to a n X k matrix (B)
            &c CPU host output purpose pointer to a m X k matrix (C) 
            to store the result
return: none
*********************************************************************
*/
void cpu_matrix_mult(int *h_a, int *h_b, int *h_result, int m, int n, int k) {
    for (int i = 0; i < m; ++i) 
    {
        for (int j = 0; j < k; ++j) 
        {
            int tmp = 0.0;
            for (int h = 0; h < n; ++h) 
            {
                tmp += h_a[i * n + h] * h_b[h * k + j];
            }
            h_result[i * k + j] = tmp;
        }
    }
}

/*
*********************************************************************
function name: main

description: test and compare

parameters: 
            none

return: none
*********************************************************************
*/
int main(int argc, char const *argv[])
{
    int m, n, k;
    /* Fixed seed for illustration */
    srand(3333);
    printf("please type in m n and k\n");
    scanf("%d %d %d", &m, &n, &k);

    // allocate memory in host RAM, h_cc is used to store CPU result
    int *h_a, *h_b, *h_c, *h_cc;
    cudaMallocHost((void **) &h_a, sizeof(int)*m*n);
    cudaMallocHost((void **) &h_b, sizeof(int)*n*k);
    cudaMallocHost((void **) &h_c, sizeof(int)*m*k);
    cudaMallocHost((void **) &h_cc, sizeof(int)*m*k);

    // random initialize matrix A
    for (int i = 0; i < m; ++i) {
        for (int j = 0; j < n; ++j) {
            h_a[i * n + j] = rand() % 1024;
        }
    }

    // random initialize matrix B
    for (int i = 0; i < n; ++i) {
        for (int j = 0; j < k; ++j) {
            h_b[i * k + j] = rand() % 1024;
        }
    }

    float gpu_elapsed_time_ms, cpu_elapsed_time_ms;

    // some events to count the execution time
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    // start to count execution time of GPU version
    cudaEventRecord(start, 0);
    // Allocate memory space on the device 
    int *d_a, *d_b, *d_c;
    cudaMalloc((void **) &d_a, sizeof(int)*m*n);
    cudaMalloc((void **) &d_b, sizeof(int)*n*k);
    cudaMalloc((void **) &d_c, sizeof(int)*m*k);

    // copy matrix A and B from host to device memory
    cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);

    unsigned int grid_rows = (m + BLOCK_SIZE - 1) / BLOCK_SIZE;
    unsigned int grid_cols = (k + BLOCK_SIZE - 1) / BLOCK_SIZE;
    dim3 dimGrid(grid_cols, grid_rows);
    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);
   
    // Launch kernel 
    if(m == n && n == k)
    {
        gpu_square_matrix_mult<<<dimGrid, dimBlock>>>(d_a, d_b, d_c, n);    
    }
    else
    {
        gpu_matrix_mult<<<dimGrid, dimBlock>>>(d_a, d_b, d_c, m, n, k);    
    }
    // Transefr results from device to host 
    cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);
    cudaThreadSynchronize();
    // time counting terminate
    cudaEventRecord(stop, 0);
    cudaEventSynchronize(stop);

    // compute time elapse on GPU computing
    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);
    printf("Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\n\n", m, n, n, k, gpu_elapsed_time_ms);

    // start the CPU version
    cudaEventRecord(start, 0);

    cpu_matrix_mult(h_a, h_b, h_cc, m, n, k);

    cudaEventRecord(stop, 0);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);
    printf("Time elapsed on matrix multiplication of %dx%d . %dx%d on CPU: %f ms.\n\n", m, n, n, k, cpu_elapsed_time_ms);

    // validate results computed by GPU
    int all_ok = 1;
    for (int i = 0; i < m; ++i)
    {
        for (int j = 0; j < k; ++j)
        {
            //printf("[%d][%d]:%d == [%d][%d]:%d, ", i, j, h_cc[i*k + j], i, j, h_c[i*k + j]);
            if(h_cc[i*k + j] != h_c[i*k + j])
            {
                all_ok = 0;
            }
        }
        //printf("\n");
    }

    // roughly compute speedup
    if(all_ok)
    {
        printf("all results are correct!!!, speedup = %f\n", cpu_elapsed_time_ms / gpu_elapsed_time_ms);
    }
    else
    {
        printf("incorrect results\n");
    }

    // free memory
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);
    cudaFreeHost(h_a);
    cudaFreeHost(h_b);
    cudaFreeHost(h_c);
    cudaFreeHost(h_cc);
    return 0;
}







matmulremote.cu

#include<cuda.h>
#include<stdio.h>

int main(void) {
    void MatrixMultiplication(float *, float *, float *, int);
    const int Width = 5;
    float M[Width*Width], N[Width*Width], P[Width*Width];
    for(int i = 0; i < (Width*Width) ; i++) {
        M[i] = 5;
        N[i] = 5;
        P[i] = 0;
    }
    MatrixMultiplication(M, N, P, Width);
    for(int i = 0; i < (Width*Width) ; i++) {
        printf("%f \n", P[i]);
    }
    int quit;
    scanf("%d",&quit);
    return 0;
}

//Matrix multiplication kernel - thread specification
__global__ void MatrixMulKernel(float *Md, float *Nd, float *Pd, int Width) {
    //2D Thread ID
    int tx = threadIdx.x;
    int ty = threadIdx.y;

    //Pvalue stores the Pd element that is computed by the thread
    float Pvalue = 0;

    for(int k = 0; k < Width ; ++k) {
        float Mdelement = Md[ty*Width + k];
        float Ndelement = Nd[k*Width + tx];
        Pvalue += (Mdelement*Ndelement);
    }

    Pd[ty*Width + tx] = Pvalue;
}

void MatrixMultiplication(float *M, float *N, float *P, int Width) {
    int size = Width*Width*sizeof(float);
    float *Md, *Nd, *Pd;

    //Transfer M and N to device memory
    cudaMalloc((void**)&Md, size);
    cudaMemcpy(Md,M,size,cudaMemcpyHostToDevice);
    cudaMalloc((void**)&Nd, size);
    cudaMemcpy(Nd,N,size,cudaMemcpyHostToDevice);

    //Allocate P on the device
    cudaMalloc((void**)&Pd,size);

    //Setup the execution configuration
    dim3 dimBlock(Width,Width);
    dim3 dimGrid(1,1);

    //Launch the device computation threads!
    MatrixMulKernel<<<dimGrid,dimBlock>>>(Md,Nd,Pd,Width);

    //Transfer P from device to host
    cudaMemcpy(P,Pd,size,cudaMemcpyDeviceToHost);

    //Free device matrices
    cudaFree(Md);
    cudaFree(Nd);
    cudaFree(Pd);
}






# pdclabcuda



sum2matrix.cu

#include "stdio.h"
#define COLUMNS 3
#define ROWS 2
__global__ void add(int *a, int *b, int *c)
{
 int x = blockIdx.x;
 int y = blockIdx.y;
 int i = (COLUMNS*y) + x;
 c[i] = a[i] + b[i];
}

/* ------------- COMPUTATION DONE ON GPU ----------------------------*/

int main()
{
 int a[ROWS][COLUMNS], b[ROWS][COLUMNS], c[ROWS][COLUMNS];
 int *dev_a, *dev_b, *dev_c;
 cudaMalloc((void **) &dev_a, ROWS*COLUMNS*sizeof(int));
 cudaMalloc((void **) &dev_b, ROWS*COLUMNS*sizeof(int));
 cudaMalloc((void **) &dev_c, ROWS*COLUMNS*sizeof(int));
 for (int y = 0; y < ROWS; y++) // Fill Arrays
 for (int x = 0; x < COLUMNS; x++)
 {
 a[y][x] = x;
 b[y][x] = y;
 }
 cudaMemcpy(dev_a, a, ROWS*COLUMNS*sizeof(int),
cudaMemcpyHostToDevice);
 cudaMemcpy(dev_b, b, ROWS*COLUMNS*sizeof(int),
cudaMemcpyHostToDevice);
 dim3 grid(COLUMNS,ROWS);
 add<<<grid,1>>>(dev_a, dev_b, dev_c);
 cudaMemcpy(c, dev_c, ROWS*COLUMNS*sizeof(int),
cudaMemcpyDeviceToHost);

/* ------------- COMPUTATION DONE ON HOST CPU ---------------------------*/

 for (int y = 0; y < ROWS; y++) // Output Arrays
 {
 for (int x = 0; x < COLUMNS; x++)
 {
 printf("[%d][%d]=%d ",y,x,c[y][x]);
 }
 printf("\n");
 }
 return 0;
}







sum2numbers.cu

#include "stdio.h"
__global__ void add(int a, int b, int *c)
{
 *c = a + b;
}
int main()
{
int a,b,c;
int *dev_c;
a=3;
b=4;
cudaMalloc((void**)&dev_c, sizeof(int));
add<<<1,1>>>(a,b,dev_c);
cudaMemcpy(&c, dev_c, sizeof(int), cudaMemcpyDeviceToHost);
printf("%d + %d is %d\n", a, b, c);
cudaFree(dev_c);
return 0;
}






sum2vectors.cu

#include "stdio.h"
#define N 10
__global__ void add(int *a, int *b, int *c)
{
 int tID = blockIdx.x;
 if (tID < N)
 {
 c[tID] = a[tID] + b[tID];
 }
}
int main()
{
 int a[N], b[N], c[N];
 int *dev_a, *dev_b, *dev_c;
 cudaMalloc((void **) &dev_a, N*sizeof(int));
 cudaMalloc((void **) &dev_b, N*sizeof(int));
 cudaMalloc((void **) &dev_c, N*sizeof(int));
 // Fill Arrays
 for (int i = 0; i < N; i++)
 {
 a[i] = i,
 b[i] = 1;
 }
 cudaMemcpy(dev_a, a, N*sizeof(int), cudaMemcpyHostToDevice);
 cudaMemcpy(dev_b, b, N*sizeof(int), cudaMemcpyHostToDevice);
 add<<<N,1>>>(dev_a, dev_b, dev_c);
 cudaMemcpy(c, dev_c, N*sizeof(int), cudaMemcpyDeviceToHost);
 for (int i = 0; i < N; i++)
 {
 printf("%d + %d = %d\n", a[i], b[i], c[i]);
 }
 return 0;
}




